{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd795f",
   "metadata": {
    "cellId": "vdnbrkztcs73vlr2ff062"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%pip uninstall transformers\n",
    "%pip install git+https://github.com/huggingface/transformers\n",
    "%pip install telebot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7239faee",
   "metadata": {
    "cellId": "xqq78a7eplalpjqhz3irci"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os, json, itertools, bisect, gc\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import transformers\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "import accelerate\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45cbdcf5",
   "metadata": {
    "cellId": "a4ketrd346leg5grakjnj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def load_model(model_name, eight_bit=0, device_map=\"auto\"):\n",
    "    global model, tokenizer, generator\n",
    "\n",
    "    print(\"Loading \"+model_name+\"...\")\n",
    "\n",
    "    if device_map == \"zero\":\n",
    "        device_map = \"balanced_low_0\"\n",
    "\n",
    "    # config\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print('gpu_count', gpu_count)\n",
    "\n",
    "    tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        #device_map=device_map,\n",
    "        #device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        #max_memory = {0: \"14GB\", 1: \"14GB\", 2: \"14GB\", 3: \"14GB\",4: \"14GB\",5: \"14GB\",6: \"14GB\",7: \"14GB\"},\n",
    "        #load_in_8bit=eight_bit,\n",
    "        #from_tf=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        load_in_8bit=False,\n",
    "        cache_dir=\"cache\"\n",
    "    ).cuda()\n",
    "\n",
    "    generator = model.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "325a0953",
   "metadata": {
    "cellId": "oo4u8jdenfk35itu61e6h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be7985e723b46a39583533d9a937907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading checkpoint shards'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./pretrained/...\n",
      "gpu_count 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "load_model(\"./pretrained/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9665afdd",
   "metadata": {
    "cellId": "yrvg5jefgzw7urigwja7i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatDoctor: I am ChatDoctor, what medical questions do you have?\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "First_chat = \"ChatDoctor: I am ChatDoctor, what medical questions do you have?\"\n",
    "print(First_chat)\n",
    "history = []\n",
    "history.append(First_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2eee8bdd",
   "metadata": {
    "cellId": "m4qm2pi10pjc5chcpevr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def go(msg=\"I'm ok\"):\n",
    "    invitation = \"ChatDoctor: \"\n",
    "    human_invitation = \"Patient: \"\n",
    "\n",
    "    history.append(human_invitation + msg)\n",
    "\n",
    "    fulltext = \"If you are a doctor, please answer the medical questions based on the patient's description. \\n\\n\" + \"\\n\\n\".join(history) + \"\\n\\n\" + invitation\n",
    "    #fulltext = \"\\n\\n\".join(history) + \"\\n\\n\" + invitation\n",
    "    \n",
    "    #print('SENDING==========')\n",
    "    #print(fulltext)\n",
    "    #print('==========')\n",
    "\n",
    "    generated_text = \"\"\n",
    "    gen_in = tokenizer(fulltext, return_tensors=\"pt\").input_ids.cuda()\n",
    "    in_tokens = len(gen_in)\n",
    "    with torch.no_grad():\n",
    "            generated_ids = generator(\n",
    "                gen_in,\n",
    "                max_new_tokens=200,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.1, # 1.0 means 'off'. unfortunately if we penalize it it will not output Sphynx:\n",
    "                temperature=0.5, # default: 1.0\n",
    "                top_k = 50, # default: 50\n",
    "                top_p = 1.0, # default: 1.0\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] # for some reason, batch_decode returns an array of one element?\n",
    "\n",
    "            text_without_prompt = generated_text[len(fulltext):]\n",
    "\n",
    "    response = text_without_prompt\n",
    "\n",
    "    response = response.split(human_invitation)[0]\n",
    "\n",
    "    response.strip()\n",
    "\n",
    "    history.append(invitation + response)\n",
    "    return invitation + response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a38be705",
   "metadata": {
    "cellId": "13vq3uk24i6eveagql94y4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "2023-06-16 13:56:24,683 (__init__.py:966 MainThread) ERROR - TeleBot: \"Infinity polling: polling exited\"\n",
      "2023-06-16 13:56:24,685 (__init__.py:968 MainThread) ERROR - TeleBot: \"Break infinity polling\"\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import telebot\n",
    "bot = telebot.TeleBot('token')\n",
    "\n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def get_text_messages(message):\n",
    "    if message.text == \"Привет\":\n",
    "        bot.send_message(message.from_user.id, \"ChatDoctor: I am ChatDoctor, what medical questions do you have?\")\n",
    "    else:\n",
    "        # forward(message)\n",
    "        msg = message.text\n",
    "        answer = go(msg)\n",
    "        bot.send_message(message.from_user.id, answer)\n",
    "\n",
    "\n",
    "bot.infinity_polling()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "b512100a-fb24-47ce-ae35-1f746896b219",
  "notebookPath": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
